[{"Title": "Quoting Ryan Broderick", "Date": "21-03-2024 - 21:49", "Day": 21, "Month": 3, "Year": 2024, "Hour": "21:49", "URL": "https://simonwillison.net/2024/Mar/21/ryan-broderick/#atom-everything", "Description_all": "<blockquote cite=\"https://www.garbageday.email/p/clout-world#a-is-impending-reputation-crisis\"><p>At this point, I\u2019m confident saying that 75% of what generative-AI text and image platforms can do is useless at best and, at worst, actively harmful. Which means that if AI companies want to onboard the millions of people they need as customers to fund themselves and bring about the great AI revolution, they\u2019ll have to perpetually outrun the millions of pathetic losers hoping to use this tech to make a quick buck. Which is something crypto has never been able to do.<br /><br />In fact, we may have already reached a point where AI images have become synonymous with scams and fraud.</p></blockquote><p class=\"cite\">&mdash; <a href=\"https://www.garbageday.email/p/clout-world#a-is-impending-reputation-crisis\">Ryan Broderick</a>", "Description": "At this point, I\u2019m confident saying that 75% of what generative-AI text and image platforms", "Feed": "Simon Willison's Weblog"}, {"Title": "How to Interpret GPT2-Small", "Date": "22-03-2024 - 00:22", "Day": 22, "Month": 3, "Year": 2024, "Hour": "00:22", "URL": "https://towardsdatascience.com/how-to-interpret-gpt2-small-76e0536a588a?source=rss----7f60cf5620c9---4", "Description_all": "<h4>Mechanistic Interpretability on prediction of repeated\u00a0tokens</h4><p>The development of large-scale language models, especially ChatGPT, has left those who have experimented with it, myself included, astonished by its remarkable linguistic prowess and its ability to accomplish diverse tasks. However, many researchers, including myself, while marveling at its capabilities, also find themselves perplexed. Despite knowing the model\u2019s architecture and the specific values of its weights, we still struggle to comprehend why a particular sequence of inputs leads to a specific sequence of\u00a0outputs.</p><p>In this blog post, I will attempt to demystify GPT2-small using mechanistic interpretability on a simple case: the prediction of repeated\u00a0tokens.</p><h3>Mechanistic Interpretability</h3><p>Traditional mathematical tools for explaining machine learning models aren\u2019t entirely suitable for language\u00a0models.</p><p>Consider SHAP, a helpful tool for explaining machine learning models. It\u2019s proficient at determining which feature significantly influenced the prediction of a good quality wine. However, it\u2019s important to remember that language models make predictions at the token level, while SHAP values are mostly computed at the feature level, making them potentially unfit for\u00a0tokens.</p><p>Moreover, Language Models (LLMs) have numerous parameters and inputs, creating a high-dimensional space. Computing SHAP values is costly even in low-dimensional spaces, and even more so in the high-dimensional space of\u00a0LLMs.</p><p>Despite tolerating the high computational costs, the explanations provided by SHAP can be superficial. For instance, knowing that the term \u201cpotter\u201d most influenced the output prediction due to the earlier mention of \u201cHarry\u201d doesn\u2019t provide much insight. It leaves us uncertain about the part of the model or the specific mechanism responsible for such a prediction.</p><p>Mechanistic Interpretability offers a different approach. It doesn\u2019t just identify important features or inputs for a model\u2019s predictions. Instead, it sheds light on the underlying mechanisms or reasoning processes, helping us understand how a model makes its predictions or decisions.</p><h3>Prediction of repeated tokens by GPT2-Small</h3><p>We will be using GPT2-small for a simple task: predicting a sequence of repeated tokens. The library we will use is <a href=\"https://neelnanda-io.github.io/TransformerLens/index.html\">TransformerLens</a>, which is designed for <a href=\"https://distill.pub/2020/circuits/zoom-in/\">mechanistic interpretability</a> of GPT-2 style language\u00a0models.</p><pre>gpt2_small: HookedTransformer = HookedTransformer.from_pretrained(&quot;gpt2-small&quot;)</pre><p>We use the code above to load the GPT2-Small model and predict tokens on a sequence generated by a specific function. This sequence includes two identical token sequences, followed by the bos_token. An example would be \u201cABCDABCD\u201d + bos_token when the seq_len is 3. For clarity, we refer to the sequence from the beginning to the seq_len as the first half, and the remaining sequence, excluding the bos_token, as the second\u00a0half.</p><pre>def generate_repeated_tokens(<br />    model: HookedTransformer, seq_len: int, batch: int = 1<br />) -&gt; Int[Tensor, &quot;batch full_seq_len&quot;]:<br />    '''<br />    Generates a sequence of repeated random tokens<br /><br />    Outputs are:<br />        rep_tokens: [batch, 1+2*seq_len]<br />    '''<br />    bos_token = (t.ones(batch, 1) * model.tokenizer.bos_token_id).long()  # generate bos token for each batch<br /><br />    rep_tokens_half = t.randint(0, model.cfg.d_vocab, (batch, seq_len), dtype=t.int64)<br />    rep_tokens = t.cat([bos_token,rep_tokens_half,rep_tokens_half], dim=-1).to(device)<br />    return rep_tokens</pre><p>When we allow the model to run on the generated token, we find an interesting observation: the model performs significantly better on the second half of the sequence than on the first half. This is measured by the log probabilities on the correct tokens. To be precise, the performance on the first half is -13.898, while the performance on the second half is\u00a0-0.644.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*YqUpWEjf8hK9G6XjAKqplA.png\" /><figcaption>Image for author: Log probs on correct\u00a0tokens</figcaption></figure><p>We can also calculate prediction accuracy, defined as the ratio of correctly predicted tokens (those identical to the generated tokens) to the total number of tokens. The accuracy for the first half sequence is 0.0, which is unsurprising since we\u2019re working with random tokens that lack actual meaning. Meanwhile, the accuracy for the second half is 0.93, significantly outperforming the first\u00a0half.</p><h3>Induction Circuits</h3><h4>Finding induction head</h4><p>The observation above might be explained by the existence of an induction circuit. This is a circuit that scans the sequence for prior instances of the current token, identifies the token that followed it previously, and predicts that the same sequence will repeat. For instance, if it encounters an \u2018A\u2019, it scans for the previous \u2018A\u2019 or a token very similar to \u2018A\u2019 in the embedding space, identifies the subsequent token \u2018B\u2019, and then predicts the next token after \u2018A\u2019 to be \u2018B\u2019 or a token very similar to \u2018B\u2019 in the embedding space.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1000/1*cg94T3U_3ApQ-yuh6zNT3Q.png\" /><figcaption>Image by author: Induction circuit</figcaption></figure><p>This prediction process can be broken down into two\u00a0steps:</p><ol><li>Identify the previous same (or similar) token. Every token in the second half of the sequence should \u201cpay attention\u201d to the token \u2018seq_len\u2019 places before it. For instance, the \u2018A\u2019 at position 4 should pay attention to the \u2018A\u2019 at position 1 if \u2018seq_len\u2019 is 3. We can call the attention head performing this task the \u201c<strong>induction head</strong>.\u201d</li><li>Identify the following token \u2018B\u2019. This is the process of copying information from the previous token (e.g., \u2018A\u2019) into the next token (e.g., \u2018B\u2019). This information will be used to \u201creproduce\u201d \u2018B\u2019 when \u2018A\u2019 appears again. We can call the attention head performing this task the \u201c<strong>previous token\u00a0head</strong>.\u201d</li></ol><p>These two heads constitute a complete induction circuit. Note that sometimes the term \u201cinduction head\u201d is also used to describe the entire \u201cinduction circuit.\u201d For more introduction of induction circuit, I highly recommend the article <a href=\"https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html\">In-context learning and induction head</a> which is a master\u00a0piece!</p><p>Now, let\u2019s identify the attention head and previous head in GPT2-small.</p><p>The following code is used to find the induction head. First, we run the model with 30 batches. Then, we calculate the mean value of the diagonal with an offset of seq_len in the attention pattern matrix. This method lets us measure the degree of attention the current token gives to the one that appears seq_len beforehand.</p><pre>def induction_score_hook(<br />    pattern: Float[Tensor, &quot;batch head_index dest_pos source_pos&quot;],<br />    hook: HookPoint,<br />):<br />    '''<br />    Calculates the induction score, and stores it in the [layer, head] position of the `induction_score_store` tensor.<br />    '''<br />    induction_stripe = pattern.diagonal(dim1=-2, dim2=-1, offset=1-seq_len) # src_pos, des_pos, one position right from seq_len<br />    induction_score = einops.reduce(induction_stripe, &quot;batch head_index position -&gt; head_index&quot;, &quot;mean&quot;)<br />    induction_score_store[hook.layer(), :] = induction_score<br /><br />seq_len = 50<br />batch = 30<br />rep_tokens_30 = generate_repeated_tokens(gpt2_small, seq_len, batch)<br />induction_score_store = t.zeros((gpt2_small.cfg.n_layers, gpt2_small.cfg.n_heads), device=gpt2_small.cfg.device)<br /><br /><br />    rep_tokens_30,<br />    return_type=None, <br />        pattern_hook_names_filter,<br />        induction_score_hook<br />    )]<br />)</pre><p>Now, let\u2019s examine the induction scores. We\u2019ll notice that some heads, such as the one on layer 5 and head 5, have a high induction score of\u00a00.91.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/900/1*YAnk4CoUXGQgyeCiLN2r8Q.png\" /><figcaption>Image by author: Induction head\u00a0scores</figcaption></figure><p>We can also display the attention pattern of this head. You will notice a clear diagonal line up to an offset of\u00a0seq_len.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/228/1*iE3_JZKil0J-LrpH_Cst-w.png\" /><figcaption>Image by author: layer 5, head 5 attention pattern</figcaption></figure><p>Similarly, we can identify the preceding token head. For instance, layer 4 head 11 demonstrates a strong pattern for the previous\u00a0token.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/900/1*KAeTFr7TBm3887wPEi0UsA.png\" /><figcaption>Image by author: previous token head\u00a0scores</figcaption></figure><h4>How do MLP layers attribute?</h4><p>Let\u2019s consider this question: do MLP layers count? We know that GPT2-Small contains both attention and MLP layers. To investigate this, I propose using an ablation technique.</p><p>Ablation, as the name implies, systematically removes certain model components and observes how performance changes as a\u00a0result.</p><p>We will replace the output of the MLP layers in the second half of the sequence with those from the first half, and observe how this affects the final loss function. We will compute the difference between the loss after replacing the MLP layer outputs and the original loss of the second half sequence using the following code.</p><pre>def patch_residual_component(<br />    residual_component,<br />    hook,<br />    pos,<br />    cache,<br />):<br />    residual_component[0,pos, :] = cache[hook.name][pos-seq_len, :]<br />    return residual_component<br /><br />ablation_scores = t.zeros((gpt2_small.cfg.n_layers, seq_len), device=gpt2_small.cfg.device)<br /><br />gpt2_small.reset_hooks()<br />logits = gpt2_small(rep_tokens, return_type=&quot;logits&quot;)<br />loss_no_ablation = cross_entropy_loss(logits[:, seq_len: max_len],rep_tokens[:, seq_len: max_len])<br /><br />for layer in tqdm(range(gpt2_small.cfg.n_layers)):<br />  for position in range(seq_len, max_len):<br />    hook_fn = functools.partial(patch_residual_component, pos=position, cache=rep_cache)<br />    ablated_logits = gpt2_small.run_with_hooks(rep_tokens, fwd_hooks=[<br />              (utils.get_act_name(&quot;mlp_out&quot;, layer), hook_fn)<br />    ])<br />    loss = cross_entropy_loss(ablated_logits[:, seq_len: max_len], rep_tokens[:, seq_len: max_len])<br />    ablation_scores[layer, position-seq_len] = loss - loss_no_ablation</pre><p>We arrive at a surprising result: aside from the first token, the ablation does not produce a significant logit difference. This suggests that the MLP layers may not have a significant contribution in the case of repeated\u00a0tokens.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/900/1*6dtICd7bWoxvBbOt2WIsKg.png\" /><figcaption>Image by author: loss different before and after ablation of mlp\u00a0layers</figcaption></figure><h3><strong>One induction circuit</strong></h3><p>Given that the MLP layers don\u2019t significantly contribute to the final prediction, we can manually construct an induction circuit using the head of layer 5, head 5, and the head of layer 4, head 11. Recall that these are the induction head and the previous token head. We do it by the following code:</p><pre>def K_comp_full_circuit(<br />    model: HookedTransformer,<br />    prev_token_layer_index: int,<br />    ind_layer_index: int,<br />    prev_token_head_index: int,<br />    ind_head_index: int<br />) -&gt; FactoredMatrix:<br />    '''<br />    Returns a (vocab, vocab)-size FactoredMatrix,<br />    with the first dimension being the query side<br />    and the second dimension being the key side (going via the previous token head)<br /><br />    '''<br />    W_E = gpt2_small.W_E<br />    W_Q = gpt2_small.W_Q[ind_layer_index, ind_head_index]<br />    W_K = model.W_K[ind_layer_index, ind_head_index]<br />    W_O = model.W_O[prev_token_layer_index, prev_token_head_index]<br />    W_V = model.W_V[prev_token_layer_index, prev_token_head_index]<br /><br />    Q = W_E @ W_Q<br />    K = W_E @ W_V @ W_O @ W_K<br />    return FactoredMatrix(Q, K.T)</pre><p>Computing the top 1 accuracy of this circuit yields a value of 0.2283. This is quite good for a circuit constructed by only two\u00a0heads!</p><p>For detailed implementation, please check my <a href=\"https://colab.research.google.com/drive/1_Qx67oPB2ZNeKa1ANYhFa9tkKGSZhX6a#scrollTo=ss1yF3e8PNYD\"><em>notebook</em></a>. And many thanks to Neel Nanda who developed the wonderful <a href=\"https://neelnanda-io.github.io/TransformerLens/index.html\">TransformerLen</a> as a great tool for Mechanistic Interpretability of\u00a0LLMs!</p><img alt=\"\" height=\"1\" src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=76e0536a588a\" width=\"1\" /><hr /><p><a href=\"https://towardsdatascience.com/how-to-interpret-gpt2-small-76e0536a588a\">How to Interpret GPT2-Small</a> was originally published in <a href=\"https://towardsdatascience.com\">Towards Data Science</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>", "Description": "Mechanistic Interpretability on prediction of repeated tokensThe development of large-scale language models, especially ChatGPT, has", "Feed": "Towards Data Science"}]